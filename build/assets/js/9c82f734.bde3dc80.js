"use strict";(self.webpackChunkphysical_ai_book=self.webpackChunkphysical_ai_book||[]).push([[721],{939(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"foundations/lesson-03-your-first-physical-ai-system","title":"Your First Physical AI System","description":"Build a complete Physical AI system from scratch - a simulated robot that perceives, reasons, acts, and learns to navigate its environment.","source":"@site/docs/foundations/lesson-03-your-first-physical-ai-system.md","sourceDirName":"foundations","slug":"/foundations/lesson-03-your-first-physical-ai-system","permalink":"/foundations/lesson-03-your-first-physical-ai-system","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Your First Physical AI System","description":"Build a complete Physical AI system from scratch - a simulated robot that perceives, reasons, acts, and learns to navigate its environment.","keywords":["physical ai project","robot simulation","hands-on robotics","python robot","autonomous navigation"],"image":"/img/og/ch01-l03-first-system.png"},"sidebar":"bookSidebar","previous":{"title":"Components of Physical AI Systems","permalink":"/foundations/lesson-02-components-of-physical-ai"},"next":{"title":"2. Perception","permalink":"/category/2-perception"}}');var i=t(4848),a=t(8453);const o={sidebar_position:3,title:"Your First Physical AI System",description:"Build a complete Physical AI system from scratch - a simulated robot that perceives, reasons, acts, and learns to navigate its environment.",keywords:["physical ai project","robot simulation","hands-on robotics","python robot","autonomous navigation"],image:"/img/og/ch01-l03-first-system.png"},r="Your First Physical AI System",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Project Overview",id:"project-overview",level:2},{value:"Project: NavigatorBot",id:"project-navigatorbot",level:3},{value:"What You&#39;ll Build",id:"what-youll-build",level:3},{value:"Step 1: Create the Environment",id:"step-1-create-the-environment",level:2},{value:"Step 2: Build the Robot",id:"step-2-build-the-robot",level:2},{value:"Step 3: Create the Simulation",id:"step-3-create-the-simulation",level:2},{value:"Step 4: Run and Test",id:"step-4-run-and-test",level:2},{value:"Expected Output",id:"expected-output",level:3},{value:"Understanding the Code",id:"understanding-the-code",level:2},{value:"The Sense-Think-Act-Learn Cycle",id:"the-sense-think-act-learn-cycle",level:3},{value:"Key Components",id:"key-components",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Summary",id:"summary",level:2},{value:"Key Takeaways",id:"key-takeaways",level:3},{value:"Checkpoint Quiz",id:"checkpoint-quiz",level:3},{value:"Exercises",id:"exercises",level:2},{value:'<span class="exercise-badge exercise-badge--basic">Basic</span> Exercise 1: Adjust Parameters',id:"basic-exercise-1-adjust-parameters",level:3},{value:'<span class="exercise-badge exercise-badge--intermediate">Intermediate</span> Exercise 2: Add a New Behavior',id:"intermediate-exercise-2-add-a-new-behavior",level:3},{value:"Further Reading",id:"further-reading",level:2},{value:"Congratulations!",id:"congratulations",level:2},{value:"What&#39;s Next?",id:"whats-next",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"your-first-physical-ai-system",children:"Your First Physical AI System"})}),"\n",(0,i.jsx)("div",{className:"lesson-progress",children:(0,i.jsxs)(n.p,{children:["\ud83d\udcd6 ",(0,i.jsx)(n.strong,{children:"Chapter 1"})," \xb7 Lesson 3 of 3 \xb7 \u23f1\ufe0f 90 minutes"]})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Build a complete Physical AI system integrating all four pillars"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Create a robot that autonomously navigates to goals while avoiding obstacles"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement a simulation environment for testing Physical AI algorithms"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Debug and troubleshoot Physical AI systems"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.p,{children:"Before starting this lesson, you should have:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Completed ",(0,i.jsx)(n.a,{href:"/docs/01-foundations/lesson-01-what-is-physical-ai",children:"Lesson 1.1: What is Physical AI?"})]}),"\n",(0,i.jsxs)(n.li,{children:["Completed ",(0,i.jsx)(n.a,{href:"/docs/01-foundations/lesson-02-components-of-physical-ai",children:"Lesson 1.2: Components of Physical AI Systems"})]}),"\n",(0,i.jsx)(n.li,{children:"Python 3.8+ installed"}),"\n",(0,i.jsx)(n.li,{children:"Basic understanding of object-oriented programming"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(n.p,{children:"You've learned about the Sense-Think-Act cycle. You've explored the four pillars of Physical AI. Now it's time to bring everything together and build something real."}),"\n",(0,i.jsxs)(n.p,{children:["In this lesson, you'll create ",(0,i.jsx)(n.strong,{children:"NavigatorBot"}),"\u2014a simulated robot that demonstrates all the principles of Physical AI. NavigatorBot will perceive obstacles using simulated sensors, reason about the best path to its goal, act by moving through its environment, and even learn from experience to improve its performance."]}),"\n",(0,i.jsx)(n.p,{children:"This isn't just a toy example. The architecture you'll build here is the same one used in real autonomous vehicles, warehouse robots, and delivery drones\u2014just at a smaller scale. By the end of this lesson, you'll have a working Physical AI system and the foundation to build much more."}),"\n",(0,i.jsx)(n.admonition,{title:"Why This Matters",type:"tip",children:(0,i.jsx)(n.p,{children:"Building a complete system helps you understand how all the pieces fit together. Real-world Physical AI development requires integrating perception, reasoning, actuation, and learning\u2014not mastering them in isolation."})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,i.jsxs)("div",{className:"project-overview",children:[(0,i.jsx)(n.h3,{id:"project-navigatorbot",children:"Project: NavigatorBot"}),(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Aspect"}),(0,i.jsx)(n.th,{children:"Details"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Duration"})}),(0,i.jsx)(n.td,{children:"90 minutes"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Difficulty"})}),(0,i.jsx)(n.td,{children:"Beginner-Intermediate"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Hardware"})}),(0,i.jsx)(n.td,{children:"None - simulation only"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Software"})}),(0,i.jsx)(n.td,{children:"Python 3.8+"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Libraries"})}),(0,i.jsx)(n.td,{children:"None (pure Python)"})]})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"what-youll-build",children:"What You'll Build"}),"\n",(0,i.jsx)(n.p,{children:"NavigatorBot is an autonomous robot that:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perceives"})," obstacles using distance sensors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reasons"})," about navigation using goal-seeking and obstacle avoidance"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Acts"})," by moving through a 2D environment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Learns"})," optimal paths through experience"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              NAVIGATORBOT ARCHITECTURE                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502                 ENVIRONMENT                       \u2502 \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2510                            \u250c\u2500\u2500\u2500\u2500\u2500\u2510      \u2502 \u2502\n\u2502  \u2502  \u2502  \u2588  \u2502 obstacles                  \u2502  \u2605  \u2502 goal \u2502 \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2518                            \u2514\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 \u2502\n\u2502  \u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2510                                  \u2502 \u2502\n\u2502  \u2502         \u2502 \ud83e\udd16  \u2502 NavigatorBot                     \u2502 \u2502\n\u2502  \u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2518                                  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                         \u2502                             \u2502\n\u2502                         \u25bc                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502  SENSE   \u2502\u2500\u2500\u2500\u25ba\u2502  THINK   \u2502\u2500\u2500\u2500\u25ba\u2502   ACT    \u2502        \u2502\n\u2502  \u2502 Sensors  \u2502    \u2502 Planning \u2502    \u2502  Motors  \u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502                         \u2502                             \u2502\n\u2502                         \u25bc                             \u2502\n\u2502                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n\u2502                  \u2502  LEARN   \u2502                         \u2502\n\u2502                  \u2502 Q-values \u2502                         \u2502\n\u2502                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n\u2502                                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"step-1-create-the-environment",children:"Step 1: Create the Environment"}),"\n",(0,i.jsx)(n.p,{children:"First, let's create the world our robot will navigate."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'"""\nNavigatorBot - Your First Physical AI System\nChapter 1, Lesson 3\n\nA complete Physical AI system demonstrating:\n- Perception (distance sensors)\n- Reasoning (navigation planning)\n- Actuation (movement control)\n- Learning (Q-learning for path optimization)\n"""\n\nimport math\nimport random\nfrom typing import List, Tuple, Dict, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n# ============================================================\n# ENVIRONMENT\n# ============================================================\n\n@dataclass\nclass Obstacle:\n    """An obstacle in the environment."""\n    x: float\n    y: float\n    radius: float\n\n    def contains(self, px: float, py: float) -> bool:\n        """Check if point (px, py) is inside this obstacle."""\n        distance = math.sqrt((px - self.x)**2 + (py - self.y)**2)\n        return distance < self.radius\n\n\nclass Environment:\n    """\n    The 2D world where our robot operates.\n\n    Features:\n    - Configurable size\n    - Multiple obstacles\n    - Goal position\n    - Collision detection\n    """\n\n    def __init__(self, width: float = 20.0, height: float = 20.0):\n        self.width = width\n        self.height = height\n        self.obstacles: List[Obstacle] = []\n        self.goal = (width - 2, height - 2)  # Default goal position\n\n    def add_obstacle(self, x: float, y: float, radius: float):\n        """Add an obstacle to the environment."""\n        self.obstacles.append(Obstacle(x, y, radius))\n\n    def add_random_obstacles(self, count: int, min_radius: float = 0.5,\n                            max_radius: float = 1.5):\n        """Add random obstacles, avoiding start and goal areas."""\n        for _ in range(count):\n            # Keep trying until we find a valid position\n            for attempt in range(100):\n                x = random.uniform(2, self.width - 2)\n                y = random.uniform(2, self.height - 2)\n                radius = random.uniform(min_radius, max_radius)\n\n                # Check if too close to start (0,0) or goal\n                dist_to_start = math.sqrt(x**2 + y**2)\n                dist_to_goal = math.sqrt((x - self.goal[0])**2 +\n                                        (y - self.goal[1])**2)\n\n                if dist_to_start > 3 and dist_to_goal > 3:\n                    # Check if overlaps with existing obstacles\n                    overlaps = False\n                    for obs in self.obstacles:\n                        dist = math.sqrt((x - obs.x)**2 + (y - obs.y)**2)\n                        if dist < radius + obs.radius + 0.5:\n                            overlaps = True\n                            break\n\n                    if not overlaps:\n                        self.add_obstacle(x, y, radius)\n                        break\n\n    def is_valid_position(self, x: float, y: float, robot_radius: float = 0.3) -> bool:\n        """Check if position is valid (in bounds and not in obstacle)."""\n        # Check bounds\n        if x < 0 or x > self.width or y < 0 or y > self.height:\n            return False\n\n        # Check obstacles\n        for obs in self.obstacles:\n            distance = math.sqrt((x - obs.x)**2 + (y - obs.y)**2)\n            if distance < obs.radius + robot_radius:\n                return False\n\n        return True\n\n    def distance_to_obstacle(self, x: float, y: float, angle: float,\n                            max_range: float = 5.0) -> float:\n        """\n        Cast a ray from (x,y) at given angle and return distance to nearest obstacle.\n        This simulates a distance sensor like ultrasonic or LIDAR.\n        """\n        # Ray marching simulation\n        step_size = 0.1\n        distance = 0\n\n        while distance < max_range:\n            # Calculate point along ray\n            check_x = x + distance * math.cos(angle)\n            check_y = y + distance * math.sin(angle)\n\n            # Check bounds\n            if check_x < 0 or check_x > self.width or check_y < 0 or check_y > self.height:\n                return distance\n\n            # Check obstacles\n            for obs in self.obstacles:\n                dist_to_obs = math.sqrt((check_x - obs.x)**2 + (check_y - obs.y)**2)\n                if dist_to_obs < obs.radius:\n                    return distance\n\n            distance += step_size\n\n        return max_range  # No obstacle found within range\n\n    def set_goal(self, x: float, y: float):\n        """Set the goal position."""\n        self.goal = (x, y)\n\n    def distance_to_goal(self, x: float, y: float) -> float:\n        """Calculate distance from a point to the goal."""\n        return math.sqrt((x - self.goal[0])**2 + (y - self.goal[1])**2)\n\n    def __str__(self):\n        return f"Environment({self.width}x{self.height}, {len(self.obstacles)} obstacles)"\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"step-2-build-the-robot",children:"Step 2: Build the Robot"}),"\n",(0,i.jsx)(n.p,{children:"Now let's create NavigatorBot with its sensors and actuators."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# ============================================================\n# NAVIGATORBOT - THE ROBOT\n# ============================================================\n\nclass Action(Enum):\n    \"\"\"Possible actions the robot can take.\"\"\"\n    FORWARD = \"forward\"\n    BACKWARD = \"backward\"\n    LEFT = \"left\"\n    RIGHT = \"right\"\n    FORWARD_LEFT = \"forward_left\"\n    FORWARD_RIGHT = \"forward_right\"\n\n\nclass NavigatorBot:\n    \"\"\"\n    A Physical AI robot that navigates to goals while avoiding obstacles.\n\n    Features:\n    - 8 distance sensors for perception\n    - Differential drive for movement\n    - Q-learning for path optimization\n    \"\"\"\n\n    def __init__(self, x: float = 1.0, y: float = 1.0, theta: float = 0.0):\n        # Position and orientation\n        self.x = x\n        self.y = y\n        self.theta = theta  # radians, 0 = facing right\n\n        # Physical properties\n        self.radius = 0.3\n        self.max_speed = 0.5  # units per step\n        self.turn_rate = math.pi / 6  # radians per step\n\n        # Sensor configuration (8 sensors evenly distributed)\n        self.num_sensors = 8\n        self.sensor_range = 5.0\n        self.sensor_angles = [i * (2 * math.pi / self.num_sensors)\n                            for i in range(self.num_sensors)]\n\n        # Learning components\n        self.q_table: Dict[Tuple, Dict[Action, float]] = {}\n        self.learning_rate = 0.1\n        self.discount_factor = 0.9\n        self.epsilon = 0.2  # Exploration rate\n\n        # Statistics\n        self.total_distance = 0\n        self.steps_taken = 0\n        self.collisions = 0\n\n    # ----------------------------------------------------------\n    # PERCEPTION: Sensing the environment\n    # ----------------------------------------------------------\n\n    def sense(self, environment: Environment) -> Dict:\n        \"\"\"\n        Gather sensor readings from the environment.\n\n        Returns:\n            Dictionary containing all sensor data\n        \"\"\"\n        sensor_readings = {}\n\n        # Distance sensors\n        for i, rel_angle in enumerate(self.sensor_angles):\n            abs_angle = self.theta + rel_angle\n            distance = environment.distance_to_obstacle(\n                self.x, self.y, abs_angle, self.sensor_range\n            )\n            sensor_readings[f'sensor_{i}'] = {\n                'distance': distance,\n                'angle': math.degrees(rel_angle),\n                'blocked': distance < self.sensor_range * 0.8\n            }\n\n        # Goal information\n        goal_distance = environment.distance_to_goal(self.x, self.y)\n        goal_dx = environment.goal[0] - self.x\n        goal_dy = environment.goal[1] - self.y\n        goal_angle = math.atan2(goal_dy, goal_dx)\n        relative_goal_angle = self._normalize_angle(goal_angle - self.theta)\n\n        sensor_readings['goal'] = {\n            'distance': goal_distance,\n            'angle': math.degrees(relative_goal_angle),\n            'reached': goal_distance < 0.5\n        }\n\n        # Current state\n        sensor_readings['pose'] = {\n            'x': self.x,\n            'y': self.y,\n            'theta': math.degrees(self.theta)\n        }\n\n        return sensor_readings\n\n    def _normalize_angle(self, angle: float) -> float:\n        \"\"\"Normalize angle to [-pi, pi].\"\"\"\n        while angle > math.pi:\n            angle -= 2 * math.pi\n        while angle < -math.pi:\n            angle += 2 * math.pi\n        return angle\n\n    # ----------------------------------------------------------\n    # REASONING: Deciding what to do\n    # ----------------------------------------------------------\n\n    def think(self, sensor_data: Dict) -> Action:\n        \"\"\"\n        Process sensor data and decide on an action.\n\n        Uses a combination of:\n        1. Reactive obstacle avoidance\n        2. Goal-directed behavior\n        3. Learned Q-values\n        \"\"\"\n        # Get discretized state for Q-learning\n        state = self._get_state(sensor_data)\n\n        # Epsilon-greedy action selection\n        if random.random() < self.epsilon:\n            # Explore: random action\n            return random.choice(list(Action))\n        else:\n            # Exploit: best learned action (with reactive override)\n            action = self._get_best_action(state, sensor_data)\n            return action\n\n    def _get_state(self, sensor_data: Dict) -> Tuple:\n        \"\"\"\n        Convert continuous sensor data to discrete state for Q-learning.\n\n        State includes:\n        - Discretized obstacle distances (front, left, right)\n        - Goal direction (left, front, right, behind)\n        \"\"\"\n        # Discretize front sensor\n        front_dist = sensor_data['sensor_0']['distance']\n        front_state = 'far' if front_dist > 2 else 'medium' if front_dist > 1 else 'close'\n\n        # Discretize left sensor (sensor 2)\n        left_dist = sensor_data['sensor_2']['distance']\n        left_state = 'far' if left_dist > 2 else 'medium' if left_dist > 1 else 'close'\n\n        # Discretize right sensor (sensor 6)\n        right_dist = sensor_data['sensor_6']['distance']\n        right_state = 'far' if right_dist > 2 else 'medium' if right_dist > 1 else 'close'\n\n        # Goal direction\n        goal_angle = sensor_data['goal']['angle']\n        if -45 <= goal_angle <= 45:\n            goal_dir = 'front'\n        elif 45 < goal_angle <= 135:\n            goal_dir = 'left'\n        elif -135 <= goal_angle < -45:\n            goal_dir = 'right'\n        else:\n            goal_dir = 'behind'\n\n        return (front_state, left_state, right_state, goal_dir)\n\n    def _get_best_action(self, state: Tuple, sensor_data: Dict) -> Action:\n        \"\"\"Get the best action based on Q-values and reactive rules.\"\"\"\n\n        # Reactive override: if obstacle very close, avoid it\n        front_dist = sensor_data['sensor_0']['distance']\n        if front_dist < 0.5:\n            # Choose direction with more space\n            left_dist = sensor_data['sensor_2']['distance']\n            right_dist = sensor_data['sensor_6']['distance']\n            if left_dist > right_dist:\n                return Action.LEFT\n            else:\n                return Action.RIGHT\n\n        # Check Q-table for learned values\n        if state in self.q_table:\n            q_values = self.q_table[state]\n            return max(q_values, key=q_values.get)\n\n        # Default: move toward goal\n        goal_angle = sensor_data['goal']['angle']\n        if abs(goal_angle) < 30:\n            return Action.FORWARD\n        elif goal_angle > 0:\n            return Action.FORWARD_LEFT\n        else:\n            return Action.FORWARD_RIGHT\n\n    # ----------------------------------------------------------\n    # ACTUATION: Executing actions\n    # ----------------------------------------------------------\n\n    def act(self, action: Action, environment: Environment) -> Tuple[bool, float]:\n        \"\"\"\n        Execute the chosen action.\n\n        Returns:\n            (success, distance_moved)\n        \"\"\"\n        # Calculate movement based on action\n        dx, dy, dtheta = 0, 0, 0\n\n        if action == Action.FORWARD:\n            dx = self.max_speed * math.cos(self.theta)\n            dy = self.max_speed * math.sin(self.theta)\n        elif action == Action.BACKWARD:\n            dx = -self.max_speed * 0.5 * math.cos(self.theta)\n            dy = -self.max_speed * 0.5 * math.sin(self.theta)\n        elif action == Action.LEFT:\n            dtheta = self.turn_rate\n        elif action == Action.RIGHT:\n            dtheta = -self.turn_rate\n        elif action == Action.FORWARD_LEFT:\n            dx = self.max_speed * 0.7 * math.cos(self.theta)\n            dy = self.max_speed * 0.7 * math.sin(self.theta)\n            dtheta = self.turn_rate * 0.5\n        elif action == Action.FORWARD_RIGHT:\n            dx = self.max_speed * 0.7 * math.cos(self.theta)\n            dy = self.max_speed * 0.7 * math.sin(self.theta)\n            dtheta = -self.turn_rate * 0.5\n\n        # Check if new position is valid\n        new_x = self.x + dx\n        new_y = self.y + dy\n        new_theta = self._normalize_angle(self.theta + dtheta)\n\n        if environment.is_valid_position(new_x, new_y, self.radius):\n            distance = math.sqrt(dx**2 + dy**2)\n            self.x = new_x\n            self.y = new_y\n            self.theta = new_theta\n            self.total_distance += distance\n            self.steps_taken += 1\n            return True, distance\n        else:\n            # Collision - don't move but update orientation\n            self.theta = new_theta\n            self.collisions += 1\n            self.steps_taken += 1\n            return False, 0\n\n    # ----------------------------------------------------------\n    # LEARNING: Improving from experience\n    # ----------------------------------------------------------\n\n    def learn(self, state: Tuple, action: Action, reward: float, next_state: Tuple):\n        \"\"\"\n        Update Q-values based on experience.\n\n        Q(s,a) = Q(s,a) + \u03b1 * (r + \u03b3 * max(Q(s',a')) - Q(s,a))\n        \"\"\"\n        # Initialize Q-values for new states\n        if state not in self.q_table:\n            self.q_table[state] = {a: 0.0 for a in Action}\n        if next_state not in self.q_table:\n            self.q_table[next_state] = {a: 0.0 for a in Action}\n\n        # Current Q-value\n        current_q = self.q_table[state][action]\n\n        # Maximum Q-value for next state\n        max_next_q = max(self.q_table[next_state].values())\n\n        # Update rule\n        new_q = current_q + self.learning_rate * (\n            reward + self.discount_factor * max_next_q - current_q\n        )\n        self.q_table[state][action] = new_q\n\n    def get_reward(self, sensor_data: Dict, action_success: bool,\n                  prev_goal_dist: float) -> float:\n        \"\"\"\n        Calculate reward for the current state.\n\n        Reward structure:\n        - Big positive for reaching goal\n        - Small positive for moving toward goal\n        - Negative for collision\n        - Small negative for each step (encourages efficiency)\n        \"\"\"\n        goal_dist = sensor_data['goal']['distance']\n\n        # Goal reached\n        if sensor_data['goal']['reached']:\n            return 100.0\n\n        # Collision\n        if not action_success:\n            return -10.0\n\n        # Progress toward goal\n        progress = prev_goal_dist - goal_dist\n        progress_reward = progress * 5  # Scale factor\n\n        # Step penalty (encourages shorter paths)\n        step_penalty = -0.1\n\n        return progress_reward + step_penalty\n\n    def reset(self, x: float = 1.0, y: float = 1.0, theta: float = 0.0):\n        \"\"\"Reset robot to starting position.\"\"\"\n        self.x = x\n        self.y = y\n        self.theta = theta\n        self.total_distance = 0\n        self.steps_taken = 0\n        self.collisions = 0\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"step-3-create-the-simulation",children:"Step 3: Create the Simulation"}),"\n",(0,i.jsx)(n.p,{children:"Now let's tie everything together with a simulation loop."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# ============================================================\n# SIMULATION\n# ============================================================\n\nclass Simulation:\n    """\n    Runs the Physical AI simulation.\n\n    Manages the sense-think-act-learn cycle and collects statistics.\n    """\n\n    def __init__(self, environment: Environment, robot: NavigatorBot):\n        self.env = environment\n        self.robot = robot\n        self.history = []\n        self.episode_rewards = []\n\n    def run_episode(self, max_steps: int = 200, verbose: bool = False) -> Dict:\n        """\n        Run one complete episode (start to goal or max steps).\n\n        Returns:\n            Dictionary with episode statistics\n        """\n        self.robot.reset()\n        self.history = [(self.robot.x, self.robot.y)]\n\n        total_reward = 0\n        goal_reached = False\n\n        for step in range(max_steps):\n            # SENSE\n            sensor_data = self.robot.sense(self.env)\n            prev_goal_dist = sensor_data[\'goal\'][\'distance\']\n            state = self.robot._get_state(sensor_data)\n\n            # Check if goal reached\n            if sensor_data[\'goal\'][\'reached\']:\n                goal_reached = True\n                if verbose:\n                    print(f"  Goal reached in {step} steps!")\n                break\n\n            # THINK\n            action = self.robot.think(sensor_data)\n\n            # ACT\n            success, distance = self.robot.act(action, self.env)\n            self.history.append((self.robot.x, self.robot.y))\n\n            # Get new state for learning\n            new_sensor_data = self.robot.sense(self.env)\n            next_state = self.robot._get_state(new_sensor_data)\n\n            # Calculate reward\n            reward = self.robot.get_reward(new_sensor_data, success, prev_goal_dist)\n            total_reward += reward\n\n            # LEARN\n            self.robot.learn(state, action, reward, next_state)\n\n            if verbose and step % 20 == 0:\n                print(f"  Step {step}: pos=({self.robot.x:.1f}, {self.robot.y:.1f}), "\n                      f"goal_dist={new_sensor_data[\'goal\'][\'distance\']:.1f}")\n\n        self.episode_rewards.append(total_reward)\n\n        return {\n            \'goal_reached\': goal_reached,\n            \'steps\': self.robot.steps_taken,\n            \'distance\': self.robot.total_distance,\n            \'collisions\': self.robot.collisions,\n            \'reward\': total_reward,\n            \'path\': self.history\n        }\n\n    def train(self, episodes: int = 100, verbose_interval: int = 10) -> List[Dict]:\n        """\n        Train the robot over multiple episodes.\n\n        Returns:\n            List of episode statistics\n        """\n        results = []\n        successes = 0\n\n        print("=" * 60)\n        print("NAVIGATORBOT TRAINING")\n        print("=" * 60)\n        print(f"Environment: {self.env}")\n        print(f"Episodes: {episodes}")\n        print("-" * 60)\n\n        for episode in range(episodes):\n            verbose = (episode % verbose_interval == 0)\n            if verbose:\n                print(f"\\nEpisode {episode + 1}/{episodes}")\n\n            result = self.run_episode(verbose=verbose)\n            results.append(result)\n\n            if result[\'goal_reached\']:\n                successes += 1\n\n            if verbose:\n                print(f"  Result: {\'SUCCESS\' if result[\'goal_reached\'] else \'TIMEOUT\'}")\n                print(f"  Steps: {result[\'steps\']}, Collisions: {result[\'collisions\']}")\n\n            # Decay exploration over time\n            self.robot.epsilon = max(0.05, self.robot.epsilon * 0.995)\n\n        print("-" * 60)\n        print(f"Training complete!")\n        print(f"Success rate: {successes}/{episodes} ({100*successes/episodes:.1f}%)")\n        print(f"States learned: {len(self.robot.q_table)}")\n\n        return results\n\n    def run_demo(self, verbose: bool = True) -> Dict:\n        """\n        Run a demonstration with no exploration (pure exploitation).\n        """\n        original_epsilon = self.robot.epsilon\n        self.robot.epsilon = 0  # No exploration\n\n        print("\\n" + "=" * 60)\n        print("NAVIGATORBOT DEMO RUN")\n        print("=" * 60)\n\n        result = self.run_episode(max_steps=300, verbose=verbose)\n\n        print("-" * 60)\n        print(f"Demo Result: {\'SUCCESS\' if result[\'goal_reached\'] else \'FAILED\'}")\n        print(f"Steps taken: {result[\'steps\']}")\n        print(f"Distance traveled: {result[\'distance\']:.2f}")\n        print(f"Collisions: {result[\'collisions\']}")\n\n        self.robot.epsilon = original_epsilon\n        return result\n\n\ndef visualize_path(environment: Environment, path: List[Tuple[float, float]]):\n    """Create a simple ASCII visualization of the robot\'s path."""\n    scale = 2\n    width = int(environment.width * scale)\n    height = int(environment.height * scale)\n\n    # Create empty grid\n    grid = [[\'.\' for _ in range(width)] for _ in range(height)]\n\n    # Draw obstacles\n    for obs in environment.obstacles:\n        ox, oy = int(obs.x * scale), int(obs.y * scale)\n        for dy in range(-int(obs.radius * scale), int(obs.radius * scale) + 1):\n            for dx in range(-int(obs.radius * scale), int(obs.radius * scale) + 1):\n                if 0 <= oy + dy < height and 0 <= ox + dx < width:\n                    if dx**2 + dy**2 <= (obs.radius * scale)**2:\n                        grid[oy + dy][ox + dx] = \'\u2588\'\n\n    # Draw path\n    for i, (x, y) in enumerate(path):\n        px, py = int(x * scale), int(y * scale)\n        if 0 <= py < height and 0 <= px < width:\n            if grid[py][px] == \'.\':\n                grid[py][px] = \'\xb7\' if i < len(path) - 1 else \'\u25cf\'\n\n    # Draw start and goal\n    gx, gy = int(environment.goal[0] * scale), int(environment.goal[1] * scale)\n    if 0 <= gy < height and 0 <= gx < width:\n        grid[gy][gx] = \'\u2605\'\n\n    sx, sy = int(path[0][0] * scale), int(path[0][1] * scale)\n    if 0 <= sy < height and 0 <= sx < width:\n        grid[sy][sx] = \'S\'\n\n    # Print grid (flip y for correct orientation)\n    print("\\nPath Visualization:")\n    print("S = Start, \u2605 = Goal, \u2588 = Obstacle, \xb7 = Path, \u25cf = End")\n    print("-" * (width + 2))\n    for row in reversed(grid):\n        print(\'|\' + \'\'.join(row) + \'|\')\n    print("-" * (width + 2))\n\n\n# ============================================================\n# MAIN PROGRAM\n# ============================================================\n\ndef main():\n    """Main entry point for NavigatorBot."""\n\n    # Create environment\n    env = Environment(width=20, height=20)\n    env.add_random_obstacles(8)\n    env.set_goal(18, 18)\n\n    # Create robot\n    robot = NavigatorBot(x=1, y=1, theta=0)\n\n    # Create simulation\n    sim = Simulation(env, robot)\n\n    # Train the robot\n    results = sim.train(episodes=50, verbose_interval=10)\n\n    # Run demo with learned behavior\n    demo_result = sim.run_demo(verbose=True)\n\n    # Visualize the path\n    if demo_result[\'goal_reached\']:\n        visualize_path(env, demo_result[\'path\'])\n\n    # Print final statistics\n    print("\\n" + "=" * 60)\n    print("FINAL STATISTICS")\n    print("=" * 60)\n    successful = sum(1 for r in results if r[\'goal_reached\'])\n    print(f"Training success rate: {successful}/{len(results)} ({100*successful/len(results):.1f}%)")\n    print(f"Unique states learned: {len(robot.q_table)}")\n    print(f"Average steps (successful): {sum(r[\'steps\'] for r in results if r[\'goal_reached\']) / max(1, successful):.1f}")\n\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"step-4-run-and-test",children:"Step 4: Run and Test"}),"\n",(0,i.jsxs)(n.p,{children:["Save all the code in a single file called ",(0,i.jsx)(n.code,{children:"navigator_bot.py"})," and run it:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python navigator_bot.py\n"})}),"\n",(0,i.jsx)(n.h3,{id:"expected-output",children:"Expected Output"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"============================================================\nNAVIGATORBOT TRAINING\n============================================================\nEnvironment: Environment(20x20, 8 obstacles)\nEpisodes: 50\n------------------------------------------------------------\n\nEpisode 1/50\n  Step 0: pos=(1.0, 1.0), goal_dist=24.0\n  Step 20: pos=(3.2, 4.1), goal_dist=20.1\n  ...\n  Result: TIMEOUT\n  Steps: 200, Collisions: 12\n\nEpisode 11/50\n  Step 0: pos=(1.0, 1.0), goal_dist=24.0\n  Step 20: pos=(5.1, 6.3), goal_dist=17.2\n  Goal reached in 87 steps!\n  Result: SUCCESS\n  Steps: 87, Collisions: 3\n...\n\n------------------------------------------------------------\nTraining complete!\nSuccess rate: 38/50 (76.0%)\nStates learned: 156\n\n============================================================\nNAVIGATORBOT DEMO RUN\n============================================================\n  Step 0: pos=(1.0, 1.0), goal_dist=24.0\n  Step 20: pos=(5.3, 5.8), goal_dist=17.5\n  Step 40: pos=(10.2, 10.1), goal_dist=11.2\n  Goal reached in 68 steps!\n------------------------------------------------------------\nDemo Result: SUCCESS\nSteps taken: 68\nDistance traveled: 28.45\nCollisions: 1\n\nPath Visualization:\nS = Start, \u2605 = Goal, \u2588 = Obstacle, \xb7 = Path, \u25cf = End\n------------------------------------------\n|......................................\u2605\u25cf|\n|.....................................\xb7\xb7\xb7|\n|....................................\xb7\xb7..|\n|...................................\xb7\xb7...|\n|..................................\xb7\xb7....|\n|.................................\xb7\xb7.....|\n|................................\u2588\u2588......|\n|...............................\u2588\u2588\xb7......|\n|..............................\xb7\xb7........|\n|.............................\xb7\xb7.........|\n|............................\xb7\xb7..........|\n|...........................\xb7\xb7...........|\n|..........................\xb7\xb7............|\n|.........................\xb7\xb7.............|\n|.............\u2588\u2588\u2588\u2588.......\xb7\xb7..............|\n|.............\u2588\u2588\u2588\u2588......\xb7\xb7...............|\n|....................\xb7\xb7\xb7.................|\n|...................\xb7\xb7...................|\n|..................\xb7\xb7....................|\n|S\xb7\xb7\xb7\xb7\xb7\xb7\xb7\xb7\xb7\xb7\xb7\xb7\xb7..\xb7\xb7......................|\n------------------------------------------\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"understanding-the-code",children:"Understanding the Code"}),"\n",(0,i.jsx)(n.p,{children:"Let's break down what's happening:"}),"\n",(0,i.jsx)(n.h3,{id:"the-sense-think-act-learn-cycle",children:"The Sense-Think-Act-Learn Cycle"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"for step in range(max_steps):\n    # SENSE: Gather environmental data\n    sensor_data = self.robot.sense(self.env)\n\n    # THINK: Decide what action to take\n    action = self.robot.think(sensor_data)\n\n    # ACT: Execute the action\n    success, distance = self.robot.act(action, self.env)\n\n    # LEARN: Update Q-values based on experience\n    self.robot.learn(state, action, reward, next_state)\n"})}),"\n",(0,i.jsx)(n.h3,{id:"key-components",children:"Key Components"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Component"}),(0,i.jsx)(n.th,{children:"Purpose"}),(0,i.jsx)(n.th,{children:"Method"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Perception"})}),(0,i.jsx)(n.td,{children:"Read distance sensors, calculate goal direction"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"robot.sense()"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Reasoning"})}),(0,i.jsx)(n.td,{children:"Choose action based on state and learned values"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"robot.think()"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Actuation"})}),(0,i.jsx)(n.td,{children:"Execute movement, handle collisions"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"robot.act()"})})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Learning"})}),(0,i.jsx)(n.td,{children:"Update Q-table based on rewards"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"robot.learn()"})})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Issue"}),(0,i.jsx)(n.th,{children:"Possible Cause"}),(0,i.jsx)(n.th,{children:"Solution"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Robot never reaches goal"}),(0,i.jsx)(n.td,{children:"Too many obstacles"}),(0,i.jsx)(n.td,{children:"Reduce obstacle count or size"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Robot gets stuck"}),(0,i.jsx)(n.td,{children:"Local minimum"}),(0,i.jsx)(n.td,{children:"Increase exploration (epsilon)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Learning doesn't improve"}),(0,i.jsx)(n.td,{children:"Wrong reward structure"}),(0,i.jsx)(n.td,{children:"Adjust reward values"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Robot oscillates"}),(0,i.jsx)(n.td,{children:"Too sensitive to goal angle"}),(0,i.jsx)(n.td,{children:"Add smoothing or hysteresis"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.h3,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u2705 Built a complete Physical AI system with all four pillars"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Implemented perception with simulated distance sensors"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Created reasoning with reactive behaviors and Q-learning"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Added actuation with collision detection"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Enabled learning through reinforcement learning"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"checkpoint-quiz",children:"Checkpoint Quiz"}),"\n",(0,i.jsxs)("div",{className:"checkpoint-quiz",children:[(0,i.jsx)(n.p,{children:"Test your understanding:"}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Why does the robot sometimes choose random actions during training?"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:'What information is included in the "state" used for Q-learning?'}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"How does the reward structure encourage efficient paths?"}),"\n"]}),"\n"]}),(0,i.jsxs)(t,{children:[(0,i.jsx)("summary",{children:"View Answers"}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Exploration (epsilon-greedy)"}),": Random actions help the robot discover new states and potentially better paths. Without exploration, the robot might get stuck in suboptimal behavior."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"State includes"}),": Discretized distances (front, left, right as 'close', 'medium', or 'far') and goal direction ('front', 'left', 'right', or 'behind'). This creates a manageable state space for Q-learning."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Reward structure"}),": Big positive for goal (+100), small positive for progress toward goal, negative for collisions (-10), and small negative for each step (-0.1). This encourages the robot to reach the goal quickly while avoiding obstacles."]}),"\n"]}),"\n"]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsxs)(n.h3,{id:"basic-exercise-1-adjust-parameters",children:[(0,i.jsx)("span",{className:"exercise-badge exercise-badge--basic",children:"Basic"})," Exercise 1: Adjust Parameters"]}),"\n",(0,i.jsx)(n.p,{children:"Experiment with different parameters:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Change the number of obstacles"}),"\n",(0,i.jsx)(n.li,{children:"Adjust learning rate and discount factor"}),"\n",(0,i.jsx)(n.li,{children:"Modify the sensor range"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Document how each change affects performance."}),"\n",(0,i.jsxs)(n.h3,{id:"intermediate-exercise-2-add-a-new-behavior",children:[(0,i.jsx)("span",{className:"exercise-badge exercise-badge--intermediate",children:"Intermediate"})," Exercise 2: Add a New Behavior"]}),"\n",(0,i.jsx)(n.p,{children:'Add a "patrol" mode where the robot visits multiple waypoints in sequence before going to the final goal.'}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Hint:"})," Modify the Environment to have a list of waypoints and update the goal as each is reached."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\ud83d\udcc4 ",(0,i.jsx)(n.a,{href:"https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/",children:"Q-Learning Algorithm Explained"})," - Deeper dive into Q-learning"]}),"\n",(0,i.jsxs)(n.li,{children:["\ud83d\udcc4 ",(0,i.jsx)(n.a,{href:"https://www.redblobgames.com/pathfinding/a-star/introduction.html",children:"Path Planning Algorithms"})," - Visual introduction to A* and pathfinding"]}),"\n",(0,i.jsxs)(n.li,{children:["\ud83d\udcf9 ",(0,i.jsx)(n.a,{href:"https://www.coursera.org/learn/robotics-motion-planning",children:"Robotics: Computational Motion Planning"})," - Free Coursera course"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"congratulations",children:"Congratulations!"}),"\n",(0,i.jsx)(n.p,{children:"You've completed Chapter 1 and built your first Physical AI system! NavigatorBot demonstrates all the core principles you'll use throughout this book:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perception"})," through sensors"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reasoning"})," through state machines and planning"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Actuation"})," through motor control"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Learning"})," through reinforcement learning"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["In the next chapter, we'll dive deeper into ",(0,i.jsx)(n.strong,{children:"Perception"})," and explore computer vision, sensor fusion, and more sophisticated sensing techniques."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"whats-next",children:"What's Next?"}),"\n",(0,i.jsxs)(n.p,{children:["Continue to ",(0,i.jsx)(n.a,{href:"/docs/02-perception/lesson-01-sensors-and-sensing",children:"Chapter 2: Perception"})," to learn how Physical AI systems see, hear, and feel the world around them."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Estimated completion time: 90 minutes"})})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>r});var s=t(6540);const i={},a=s.createContext(i);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);